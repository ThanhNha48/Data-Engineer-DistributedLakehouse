# ğŸš€ Modern E-commerce Data Lakehouse
## ğŸ“Œ Overview

The **Modern E-commerce Data Lakehouse** project builds an **end-to-end data processing and analytics system** for e-commerce data, based on a **modern Data Lakehouse architecture**.

The system follows the **Medallion Architecture (Bronze â€“ Silver â€“ Gold)**, enabling:

* Clear separation between raw, cleaned, and analytics-ready data
* Clean, consistent, and historical data tracking
* Data readiness for analytics, reporting, and BI use cases

---

## ğŸ¯ Project Objectives

* Build an **end-to-end data pipeline** for e-commerce data
* Ingest and process large-scale data in a **scalable and efficient** manner
* Ensure **data quality** throughout the pipeline
* Store analytics data using **Star Schema modeling**
* Track historical data changes using **Slowly Changing Dimension (SCD) Type 2**
* Enable **high-performance SQL analytics** on a Lakehouse platform

---

## ğŸ—ï¸ High-Level Architecture

```text
Seed / Raw Data
      â†“
Apache Spark
      â†“
Bronze Layer (Iceberg)
      â†“
dbt (Cleaning & Standardization)
      â†“
Silver Layer (Iceberg)
      â†“
dbt (Star Schema + SCD Type 2)
      â†“
Gold Layer (Iceberg)
      â†“
Trino / BI / Analytics
```

---

## ğŸ§© Core Components

* **Apache Spark** â€“ Distributed data ingestion and processing
* **Apache Iceberg** â€“ Lakehouse table format
* **dbt** â€“ Data transformation and modeling
* **Apache Airflow** â€“ Workflow orchestration
* **MinIO** â€“ S3-compatible object storage
* **Project Nessie** â€“ Versioned metadata management for Iceberg
* **Trino** â€“ Distributed SQL query engine for analytics

---

## â­ Star Schema Design (Gold Layer)

The Gold layer is modeled using a **Star Schema**, optimized for analytical workloads and BI tools.

![Star Schema Design](./drawio/DesignStarSchema.jpg)

---

## â–¶ï¸ Usage Guide

### 1ï¸âƒ£ Prerequisites

Make sure the following tools are installed:

* Docker & Docker Compose
* Git

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/modern-ecommerce-data-lakehouse.git
cd modern-ecommerce-data-lakehouse
```

---

### 3ï¸âƒ£ Start the Infrastructure

```bash
docker-compose up -d
```
---

### 4ï¸âƒ£ Service Configuration

#### ğŸ”¹ MinIO

* URL: [http://localhost:9001](http://localhost:9001)
* Create a bucket: **warehouse**

#### ğŸ”¹ Apache Airflow

* URL: [http://localhost:8082](http://localhost:8080)
* Enable and trigger the DAG: **`ecommerce_dag_pipeline`**

Once triggered, the pipeline will automatically execute the full workflow from **Bronze â†’ Silver â†’ Gold**.


